{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzy/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "from copy import deepcopy\n",
    "from anndata import AnnData\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from einops import einsum, rearrange, repeat\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set deterministic CUDA ops\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {}.'.format('GPU' if device == 'cuda' else 'CPU (this may be much slower)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################计算相对位置relative_position方法#######################################\n",
    "\n",
    "\"\"\"\n",
    "位置信息的角度值（positional thetas）在头维度（head_dim）的每个值上都是不同的，遵循论文中规定的方法。这些角度值用于在保留的并行和循环形式中更新位置嵌入。\n",
    "实际的角度值在论文中没有具体指定，因此从官方实现中复制了这些值。\n",
    "\"\"\"\n",
    "def positionThetas(head_dim, scale = 10000, device = \"cuda\"):\n",
    "    x = torch.linspace(0, 1, steps=head_dim // 2, device=device)\n",
    "    thetas = 1 / (scale**x)\n",
    "    return repeat(thetas, \"d -> (d n)\", n=2)\n",
    "\n",
    "def multiplyByi(x):\n",
    "    return torch.stack((-x[..., 1::2], x[..., ::2]), dim=-1).flatten(start_dim=-2)\n",
    "\n",
    "def thetaShift(x, sin, cos):\n",
    "    return (x * cos) + (multiplyByi(x) * sin)\n",
    "###########################################################################################################\n",
    "\n",
    "####################################计算并行retention_parallel#############################################\n",
    "\"\"\"\n",
    "在 RetNet（保留网络）中，每个保留头部（retention head）的衰减值是不同的，这是按照论文中规定的方法进行的。这里的“衰减值”通常指的是用于计算衰减系数（decay coefficients）的值。\n",
    "在概念上，作者认为每个头部都有一个不同的“保留窗口”，这是头部可以回顾的过去时间步数的有效数量。这个“保留窗口”表示头部可以关注的时间跨度，而这个时间跨度实际上由衰减系数来决定。\n",
    "每个头部有一个衰减系数，这个系数决定了该头部可以在过去的时间范围内关注的步数。较大的衰减系数将导致较短的保留窗口，头部能够关注的时间跨度较小；较小的衰减系数则会导致更长的保留窗口，头部可以关注更远的时间跨度。\n",
    "衰减系数的调整是为了控制不同头部的关注范围，以适应不同的任务需求和数据特性。\n",
    "\"\"\"\n",
    "def calDecayGammas(num_heads, device):\n",
    "    xmin, xmax = log(1 / 32), log(1 / 512)\n",
    "    x = torch.linspace(xmin, xmax, steps=num_heads, device=device)\n",
    "    return 1 - torch.exp(x)\n",
    "\n",
    "\"\"\"\n",
    "“衰减掩码是使得并行保留等效于循环保留的关键组成部分之一。衰减系数会被预先计算，并一次性应用于相似性矩阵，而不是像在循环保留的形式中逐个元素地应用。\n",
    "\"\"\"\n",
    "def calDecayMask(q_len, k_len, decay_gammas, device):\n",
    "    q_pos = torch.arange(q_len, device=device)\n",
    "    k_pos = torch.arange(k_len, device=device)\n",
    "    \n",
    "    distance = torch.abs(q_pos.unsqueeze(-1) - k_pos.unsqueeze(0)).float()\n",
    "\n",
    "    # 将上三角距离设置为无穷大，这样只有过去的键才能影响当前的查询。 （将距离设置为无穷大确保在这些位置上衰减矩阵为0，因为在 -1 < x < 1 时，x^(inf) = 0。\n",
    "    distance_mask = torch.ones_like(distance, dtype=torch.bool).triu_(diagonal=1)\n",
    "    distance = distance.masked_fill(distance_mask, float(\"inf\"))\n",
    "    \n",
    "    distance = rearrange(distance, \"n s -> () n s\")\n",
    "    decay_gammas = rearrange(decay_gammas, \"h -> h () ()\")\n",
    "    return decay_gammas**distance\n",
    "\n",
    "\"\"\"\n",
    "计算并行retention\n",
    "\"\"\"\n",
    "def retention_parallel(q,k,v):\n",
    "    decay_gammas = calDecayGammas(num_heads=q.shape[1], device=q.device)\n",
    "    decay_mask = calDecayMask(q_len=q.shape[2], k_len=k.shape[2], decay_gammas=decay_gammas, device=q.device)\n",
    "    \n",
    "    scale = k.size(-1) ** 0.5\n",
    "    k = k / scale\n",
    "    \n",
    "    similarity = einsum(q, k, \"b h n d, b h s d -> b h n s\")\n",
    "    similarity = similarity * rearrange(decay_mask, \"h n s -> () h n s\")\n",
    "    retention = einsum(similarity, v, \"b h n s, b h s d -> b h n d\")\n",
    "    \n",
    "    return retention, None\n",
    "\n",
    "###########################################################################################################\n",
    "\n",
    "class MultiScaleRetention(torch.nn.Module):\n",
    "    def __init__(self,embedding_dim = 320, num_heads = 4, dropout = 0.1 , activation = \"swish\", group_norm_eps = 1e-6, device = \"cuda\", bias = True):\n",
    "        super(MultiScaleRetention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.activation = torch.nn.functional.silu\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        \n",
    "        self.q_projection = torch.nn.Linear(embedding_dim, embedding_dim, bias=bias, device=device)\n",
    "        self.k_projection = torch.nn.Linear(embedding_dim, embedding_dim, bias=bias, device=device)\n",
    "        self.v_projection = torch.nn.Linear(embedding_dim, embedding_dim, bias=bias, device=device)\n",
    "        self.group_norm = torch.nn.GroupNorm(num_groups=num_heads, num_channels=num_heads, affine=False, eps=group_norm_eps, device=device)\n",
    "        self.g_projection = torch.nn.Linear(embedding_dim, embedding_dim, bias=bias, device=device)\n",
    "        self.out_projection = torch.nn.Linear(embedding_dim, embedding_dim, bias=bias, device=device)\n",
    "        \n",
    "        thetas = positionThetas(head_dim = self.head_dim, device=device)\n",
    "        self.register_buffer(\"thetas\", thetas)\n",
    "        self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        torch.nn.init.xavier_normal_(self.q_projection.weight)\n",
    "        if self.q_projection.bias is not None:\n",
    "            torch.nn.init.constant_(self.q_projection.bias, 0)\n",
    "        torch.nn.init.xavier_normal_(self.k_projection.weight)\n",
    "        if self.k_projection.bias is not None:\n",
    "            torch.nn.init.constant_(self.k_projection.bias, 0)\n",
    "        torch.nn.init.xavier_normal_(self.v_projection.weight)\n",
    "        if self.v_projection.bias is not None:\n",
    "            torch.nn.init.constant_(self.v_projection.bias, 0)\n",
    "        torch.nn.init.xavier_normal_(self.g_projection.weight)\n",
    "        if self.g_projection.bias is not None:\n",
    "            torch.nn.init.constant_(self.g_projection.bias, 0)\n",
    "        torch.nn.init.xavier_normal_(self.out_projection.weight)\n",
    "        if self.out_projection.bias is not None:\n",
    "            torch.nn.init.constant_(self.out_projection.bias, 0)\n",
    "    \n",
    "    # parallel并行训练\n",
    "    def forward(self, query, k, v):\n",
    "        q = self.q_projection(query)\n",
    "        k = self.k_projection(k)\n",
    "        v = self.v_projection(v)\n",
    "        \n",
    "        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        \n",
    "        # 计算相对位置 relative_position\n",
    "        indices = torch.arange(q.size(2), device=q.device)\n",
    "        indices = rearrange(indices, \"n -> () () n ()\")\n",
    "        thetas = rearrange(self.thetas, \"d -> () () () d\")\n",
    "        angles = indices * thetas\n",
    "        sin = torch.sin(angles)\n",
    "        cos = torch.cos(angles)\n",
    "        q = thetaShift(q, sin, cos)\n",
    "        k = thetaShift(k, sin, cos)\n",
    "        \n",
    "        retention, weights = retention_parallel(q, k, v)\n",
    "        \n",
    "        # 为了以与循环形式等效的方式应用分组归一化，我们将序列维度折叠到批次维度中。否则，归一化将在整个输入序列上应用。\n",
    "        batch_size = retention.size(0)\n",
    "        retention = rearrange(retention, \"b h n d -> (b n) h d\")\n",
    "        retention = torch.nn.functional.dropout(retention, p=self.dropout, training=self.training)\n",
    "        retention = self.group_norm(retention)\n",
    "        retention = rearrange(retention, \"(b n) h d -> b n (h d)\", b=batch_size)\n",
    "        \n",
    "        # 与多头注意力不同，保留机制论文应用了 \"swish\" 门，以增加模型的非线性容量。（在我看来，这很可能是为了弥补保留机制中缺少 \"softmax\" 激活的不足。）\n",
    "        gate = self.activation(self.g_projection(query))\n",
    "        retention = self.out_projection(retention * gate)\n",
    "        \n",
    "        return retention, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "主要来自于 'torch.nn.TransformerDecoderLayer'，但有所变化：\n",
    "    使用 MultiScaleRetention 替代 MultiheadAttention\n",
    "    没有交叉注意力层，因为保留机制与其不兼容\n",
    "\"\"\"\n",
    "\n",
    "class RetNetLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim = 320, num_heads = 4, dim_feedforward = 1024, dropout = 0.1, layer_norm_eps = 1e-6, device = \"cuda\"):\n",
    "        super(RetNetLayer,self).__init__()\n",
    "        self.activation = torch.nn.functional.silu\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.layernorm1 = torch.nn.LayerNorm(embedding_dim, eps=layer_norm_eps, device=device)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(embedding_dim, eps=layer_norm_eps, device=device)\n",
    "        self.retention = MultiScaleRetention(embedding_dim=embedding_dim, num_heads=num_heads, dropout=dropout, device=device)\n",
    "        self.linear1 = torch.nn.Linear(embedding_dim, dim_feedforward, device=device)\n",
    "        self.linear2 = torch.nn.Linear(dim_feedforward, embedding_dim, device=device)\n",
    "        self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        torch.nn.init.xavier_normal_(self.linear1.weight)\n",
    "        torch.nn.init.constant_(self.linear1.bias, 0)\n",
    "        torch.nn.init.xavier_normal_(self.linear2.weight)\n",
    "        torch.nn.init.constant_(self.linear2.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_tmp = self.layernorm1(x)\n",
    "        x_tmp, _ = self.retention(x_tmp, x_tmp, x_tmp)\n",
    "        x_tmp = self.dropout(x_tmp)\n",
    "        x = x + x_tmp\n",
    "        \n",
    "        x_tmp = self.layernorm2(x)\n",
    "        x_tmp = self.activation(self.linear1(x_tmp))\n",
    "        x_tmp = self.dropout(x_tmp)\n",
    "        x_tmp = self.linear2(x_tmp)\n",
    "        x_tmp = self.dropout(x_tmp)\n",
    "        x = x + x_tmp\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class RetNetBlock(torch.nn.Module):\n",
    "    def __init__(self, retnetLayers, num_layers):\n",
    "        super(RetNetBlock,self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = torch.nn.ModuleList([deepcopy(retnetLayers) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetNet(torch.nn.Module):\n",
    "    def __init__(self,vocbal_size = 33 ,seq_len = 1024, embedding_dim = 320, num_heads = 4, num_layers = 3, device = \"cuda\", dtype = None, \n",
    "                 dropout = 0.1, activation = \"swish\", dim_feedforward = 1024, norm_first = True,  layer_norm_eps = 1e-6):\n",
    "        super(RetNet,self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = torch.nn.Embedding(seq_len, embedding_dim)\n",
    "        \n",
    "        retnetLayer = RetNetLayer()\n",
    "        self.block = RetNetBlock(retnetLayer, num_layers)\n",
    "        \n",
    "        self.output = torch.nn.Linear(embedding_dim, vocbal_size, device=device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.block(x)\n",
    "        #x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjHead(torch.nn.Module):\n",
    "    def __init__(self,in_dim = 640, hid_dim = 320, out_dim = 33, droupout = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim,hid_dim,bias=True),\n",
    "            torch.nn.Dropout(droupout,inplace=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hid_dim,out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self,combined_embedding):\n",
    "        outputs = self.layer(combined_embedding)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class  SARSCoV2ESM2(torch.nn.Module):\n",
    "    def __init__(self,esm2,retnet,isEval = False):\n",
    "        super(SARSCoV2ESM2, self).__init__()\n",
    "        self.esm2 = esm2\n",
    "        self.retnet = retnet\n",
    "        self.head = ProjHead()\n",
    "        self.proj = torch.nn.Linear(640,320)\n",
    "        self.isEval = isEval\n",
    "    def forward(self,x):\n",
    "        x_dict = {'input_ids': x.to(device), 'attention_mask': torch.ones(len(x), 225).to(device)}\n",
    "        esm_outputs = self.esm2(**x_dict).last_hidden_state\n",
    "        retnet_outputs = self.retnet(x.to(device))\n",
    "        combined_embedding = torch.cat((esm_outputs,retnet_outputs),dim=2)\n",
    "        \n",
    "        if self.isEval:\n",
    "            outputs = self.proj(combined_embedding)\n",
    "        else:\n",
    "            outputs = self.head(combined_embedding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzy/miniconda3/lib/python3.10/site-packages/Bio/Seq.py:175: BiopythonWarning: Biopython Seq objects now use string comparison. Older versions of Biopython used object comparison. During this transition, please use hash(id(my_seq)) or my_dict[id(my_seq)] if you want the old behaviour, or use hash(str(my_seq)) or my_dict[str(my_seq)] for the new string hashing behaviour.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "aa_sequences_dict = {}\n",
    "aa_sequences_list = []\n",
    "\n",
    "for record in SeqIO.parse('./generateseqs.fasta', 'fasta'):\n",
    "    aa_sequences_dict[record.seq] = [] \n",
    "    \n",
    "    seqId = record.description.split('|')[0]\n",
    "    seq_info_dict = {\n",
    "        'seqId' : seqId,\n",
    "        'isNew' : 'new',\n",
    "        'year' : 'None' \n",
    "    } \n",
    "\n",
    "        \n",
    "    aa_sequences_dict[record.seq].append(seq_info_dict)\n",
    "\n",
    "for seq in list(aa_sequences_dict.keys()):\n",
    "    aa_sequences_list.append(str(seq))\n",
    "\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "esm2 = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "seq_encode = tokenizer_(aa_sequences_list,return_tensors=\"pt\")[\"input_ids\"]\n",
    "seqs = [seq for seq in seq_encode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retnet = RetNet(seq_len = 225, embedding_dim = 320, num_heads = 4, num_layers = 3, device = \"cuda\")\n",
    "model = SARSCoV2ESM2(esm2,retnet,True).to(device)\n",
    "model = torch.load('./model_final.pth')\n",
    "batch_size = 1000\n",
    "data_loader = torch.utils.data.DataLoader(dataset=seq_encode, batch_size=batch_size)\n",
    "seq_embedding = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch_data in enumerate(data_loader):\n",
    "        output = model(batch_data).mean(1)\n",
    "        seq_embedding.append(output)\n",
    "\n",
    "seq_embeddings = torch.cat(seq_embedding, dim=0)\n",
    "seq_embeddings = seq_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id ,seq in enumerate(aa_sequences_dict):\n",
    "    for seq_info_dict in aa_sequences_dict[seq]:\n",
    "        seq_info_dict['embedding'] = seq_embeddings[id]\n",
    "\n",
    "from collections import Counter\n",
    "cluster_infos = {}\n",
    "for seq in aa_sequences_dict:\n",
    "    seq_info_dict = aa_sequences_dict[seq][0]\n",
    "    for key in seq_info_dict:\n",
    "        if key == 'embedding':\n",
    "            continue\n",
    "        if key not in cluster_infos:\n",
    "            cluster_infos[key] = []\n",
    "        cluster_infos[key].append(Counter([seq_info_dict[key] for seq_info_dict in aa_sequences_dict[seq]]).most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "\n",
    "adata = AnnData(seq_embeddings) \n",
    "for key in cluster_infos:\n",
    "    adata.obs[key] = cluster_infos[key]\n",
    "sc.pp.neighbors(adata, n_neighbors=15, use_rep='X')\n",
    "#sc.tl.louvain(adata, resolution=2.5)\n",
    "sc.tl.leiden(adata, resolution=0.3)\n",
    "sc.tl.umap(adata, min_dist=1) \n",
    "#sc.pl.umap(adata, color='louvain', ax=ax)\n",
    "sc.pl.umap(adata, color='leiden', ax=ax,size = 3)\n",
    "#sc.pl.umap(adata, color='year', ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
