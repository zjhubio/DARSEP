{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzy/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import evolocity as evo\n",
    "from Bio import pairwise2\n",
    "from copy import deepcopy\n",
    "from anndata import AnnData\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import einsum, rearrange, repeat\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set deterministic CUDA ops\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {}.'.format('GPU' if device == 'cuda' else 'CPU (this may be much slower)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################计算相对位置relative_position方法#######################################\n",
    "\n",
    "\"\"\"\n",
    "位置信息的角度值（positional thetas）在头维度（head_dim）的每个值上都是不同的，遵循论文中规定的方法。这些角度值用于在保留的并行和循环形式中更新位置嵌入。\n",
    "实际的角度值在论文中没有具体指定，因此从官方实现中复制了这些值。\n",
    "\"\"\"\n",
    "def positionThetas(head_dim, scale = 10000, device = \"cuda\"):\n",
    "    x = torch.linspace(0, 1, steps=head_dim // 2, device=device)\n",
    "    thetas = 1 / (scale**x)\n",
    "    return repeat(thetas, \"d -> (d n)\", n=2)\n",
    "\n",
    "def multiplyByi(x):\n",
    "    return torch.stack((-x[..., 1::2], x[..., ::2]), dim=-1).flatten(start_dim=-2)\n",
    "\n",
    "def thetaShift(x, sin, cos):\n",
    "    return (x * cos) + (multiplyByi(x) * sin)\n",
    "###########################################################################################################\n",
    "\n",
    "####################################计算并行retention_parallel#############################################\n",
    "\"\"\"\n",
    "在 RetNet（保留网络）中，每个保留头部（retention head）的衰减值是不同的，这是按照论文中规定的方法进行的。这里的“衰减值”通常指的是用于计算衰减系数（decay coefficients）的值。\n",
    "在概念上，作者认为每个头部都有一个不同的“保留窗口”，这是头部可以回顾的过去时间步数的有效数量。这个“保留窗口”表示头部可以关注的时间跨度，而这个时间跨度实际上由衰减系数来决定。\n",
    "每个头部有一个衰减系数，这个系数决定了该头部可以在过去的时间范围内关注的步数。较大的衰减系数将导致较短的保留窗口，头部能够关注的时间跨度较小；较小的衰减系数则会导致更长的保留窗口，头部可以关注更远的时间跨度。\n",
    "衰减系数的调整是为了控制不同头部的关注范围，以适应不同的任务需求和数据特性。\n",
    "\"\"\"\n",
    "def calDecayGammas(num_heads, device):\n",
    "    xmin, xmax = log(1 / 32), log(1 / 512)\n",
    "    x = torch.linspace(xmin, xmax, steps=num_heads, device=device)\n",
    "    return 1 - torch.exp(x)\n",
    "\n",
    "\"\"\"\n",
    "“衰减掩码是使得并行保留等效于循环保留的关键组成部分之一。衰减系数会被预先计算，并一次性应用于相似性矩阵，而不是像在循环保留的形式中逐个元素地应用。\n",
    "\"\"\"\n",
    "def calDecayMask(q_len, k_len, decay_gammas, device):\n",
    "    q_pos = torch.arange(q_len, device=device)\n",
    "    k_pos = torch.arange(k_len, device=device)\n",
    "    \n",
    "    distance = torch.abs(q_pos.unsqueeze(-1) - k_pos.unsqueeze(0)).float()\n",
    "\n",
    "    # 将上三角距离设置为无穷大，这样只有过去的键才能影响当前的查询。 （将距离设置为无穷大确保在这些位置上衰减矩阵为0，因为在 -1 < x < 1 时，x^(inf) = 0。\n",
    "    distance_mask = torch.ones_like(distance, dtype=torch.bool).triu_(diagonal=1)\n",
    "    distance = distance.masked_fill(distance_mask, float(\"inf\"))\n",
    "    \n",
    "    distance = rearrange(distance, \"n s -> () n s\")\n",
    "    decay_gammas = rearrange(decay_gammas, \"h -> h () ()\")\n",
    "    return decay_gammas**distance\n",
    "\n",
    "\"\"\"\n",
    "计算并行retention\n",
    "\"\"\"\n",
    "def retention_parallel(q,k,v):\n",
    "    decay_gammas = calDecayGammas(num_heads=q.shape[1], device=q.device)\n",
    "    decay_mask = calDecayMask(q_len=q.shape[2], k_len=k.shape[2], decay_gammas=decay_gammas, device=q.device)\n",
    "    \n",
    "    scale = k.size(-1) ** 0.5\n",
    "    k = k / scale\n",
    "    \n",
    "    similarity = einsum(q, k, \"b h n d, b h s d -> b h n s\")\n",
    "    similarity = similarity * rearrange(decay_mask, \"h n s -> () h n s\")\n",
    "    retention = einsum(similarity, v, \"b h n s, b h s d -> b h n d\")\n",
    "    \n",
    "    return retention, None\n",
    "\n",
    "###########################################################################################################\n",
    "\n",
    "class MultiScaleRetention(torch.nn.Module):\n",
    "    def __init__(self,embedding_dim = 320, num_heads = 4, dropout = 0.1 , activation = \"swish\", group_norm_eps = 1e-6, device = \"cuda\", bias = True):\n",
    "        super(MultiScaleRetention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.activation = torch.nn.functional.silu\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        \n",
    "        self.q_projection = torch.nn.Linear(embedding_dim, embedding_dim, bias=bias, device=device)\n",
    "        self.k_projection = torch.nn.Linear(embedding_dim, embedding_dim, bias=bias, device=device)\n",
    "        self.v_projection = torch.nn.Linear(embedding_dim, embedding_dim, bias=bias, device=device)\n",
    "        self.group_norm = torch.nn.GroupNorm(num_groups=num_heads, num_channels=num_heads, affine=False, eps=group_norm_eps, device=device)\n",
    "        self.g_projection = torch.nn.Linear(embedding_dim, embedding_dim, bias=bias, device=device)\n",
    "        self.out_projection = torch.nn.Linear(embedding_dim, embedding_dim, bias=bias, device=device)\n",
    "        \n",
    "        thetas = positionThetas(head_dim = self.head_dim, device=device)\n",
    "        self.register_buffer(\"thetas\", thetas)\n",
    "        self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        torch.nn.init.xavier_normal_(self.q_projection.weight)\n",
    "        if self.q_projection.bias is not None:\n",
    "            torch.nn.init.constant_(self.q_projection.bias, 0)\n",
    "        torch.nn.init.xavier_normal_(self.k_projection.weight)\n",
    "        if self.k_projection.bias is not None:\n",
    "            torch.nn.init.constant_(self.k_projection.bias, 0)\n",
    "        torch.nn.init.xavier_normal_(self.v_projection.weight)\n",
    "        if self.v_projection.bias is not None:\n",
    "            torch.nn.init.constant_(self.v_projection.bias, 0)\n",
    "        torch.nn.init.xavier_normal_(self.g_projection.weight)\n",
    "        if self.g_projection.bias is not None:\n",
    "            torch.nn.init.constant_(self.g_projection.bias, 0)\n",
    "        torch.nn.init.xavier_normal_(self.out_projection.weight)\n",
    "        if self.out_projection.bias is not None:\n",
    "            torch.nn.init.constant_(self.out_projection.bias, 0)\n",
    "    \n",
    "    # parallel并行训练\n",
    "    def forward(self, query, k, v):\n",
    "        q = self.q_projection(query)\n",
    "        k = self.k_projection(k)\n",
    "        v = self.v_projection(v)\n",
    "        \n",
    "        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        \n",
    "        # 计算相对位置 relative_position\n",
    "        indices = torch.arange(q.size(2), device=q.device)\n",
    "        indices = rearrange(indices, \"n -> () () n ()\")\n",
    "        thetas = rearrange(self.thetas, \"d -> () () () d\")\n",
    "        angles = indices * thetas\n",
    "        sin = torch.sin(angles)\n",
    "        cos = torch.cos(angles)\n",
    "        q = thetaShift(q, sin, cos)\n",
    "        k = thetaShift(k, sin, cos)\n",
    "        \n",
    "        retention, weights = retention_parallel(q, k, v)\n",
    "        \n",
    "        # 为了以与循环形式等效的方式应用分组归一化，我们将序列维度折叠到批次维度中。否则，归一化将在整个输入序列上应用。\n",
    "        batch_size = retention.size(0)\n",
    "        retention = rearrange(retention, \"b h n d -> (b n) h d\")\n",
    "        retention = torch.nn.functional.dropout(retention, p=self.dropout, training=self.training)\n",
    "        retention = self.group_norm(retention)\n",
    "        retention = rearrange(retention, \"(b n) h d -> b n (h d)\", b=batch_size)\n",
    "        \n",
    "        # 与多头注意力不同，保留机制论文应用了 \"swish\" 门，以增加模型的非线性容量。（在我看来，这很可能是为了弥补保留机制中缺少 \"softmax\" 激活的不足。）\n",
    "        gate = self.activation(self.g_projection(query))\n",
    "        retention = self.out_projection(retention * gate)\n",
    "        \n",
    "        return retention, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "主要来自于 'torch.nn.TransformerDecoderLayer'，但有所变化：\n",
    "    使用 MultiScaleRetention 替代 MultiheadAttention\n",
    "    没有交叉注意力层，因为保留机制与其不兼容\n",
    "\"\"\"\n",
    "\n",
    "class RetNetLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim = 320, num_heads = 4, dim_feedforward = 1024, dropout = 0.1, layer_norm_eps = 1e-6, device = \"cuda\"):\n",
    "        super(RetNetLayer,self).__init__()\n",
    "        self.activation = torch.nn.functional.silu\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.layernorm1 = torch.nn.LayerNorm(embedding_dim, eps=layer_norm_eps, device=device)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(embedding_dim, eps=layer_norm_eps, device=device)\n",
    "        self.retention = MultiScaleRetention(embedding_dim=embedding_dim, num_heads=num_heads, dropout=dropout, device=device)\n",
    "        self.linear1 = torch.nn.Linear(embedding_dim, dim_feedforward, device=device)\n",
    "        self.linear2 = torch.nn.Linear(dim_feedforward, embedding_dim, device=device)\n",
    "        self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        torch.nn.init.xavier_normal_(self.linear1.weight)\n",
    "        torch.nn.init.constant_(self.linear1.bias, 0)\n",
    "        torch.nn.init.xavier_normal_(self.linear2.weight)\n",
    "        torch.nn.init.constant_(self.linear2.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_tmp = self.layernorm1(x)\n",
    "        x_tmp, _ = self.retention(x_tmp, x_tmp, x_tmp)\n",
    "        x_tmp = self.dropout(x_tmp)\n",
    "        x = x + x_tmp\n",
    "        \n",
    "        x_tmp = self.layernorm2(x)\n",
    "        x_tmp = self.activation(self.linear1(x_tmp))\n",
    "        x_tmp = self.dropout(x_tmp)\n",
    "        x_tmp = self.linear2(x_tmp)\n",
    "        x_tmp = self.dropout(x_tmp)\n",
    "        x = x + x_tmp\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class RetNetBlock(torch.nn.Module):\n",
    "    def __init__(self, retnetLayers, num_layers):\n",
    "        super(RetNetBlock,self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = torch.nn.ModuleList([deepcopy(retnetLayers) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetNet(torch.nn.Module):\n",
    "    def __init__(self,vocbal_size = 33 ,seq_len = 1024, embedding_dim = 320, num_heads = 4, num_layers = 3, device = \"cuda\", dtype = None, \n",
    "                 dropout = 0.1, activation = \"swish\", dim_feedforward = 1024, norm_first = True,  layer_norm_eps = 1e-6):\n",
    "        super(RetNet,self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = torch.nn.Embedding(seq_len, embedding_dim)\n",
    "        \n",
    "        retnetLayer = RetNetLayer()\n",
    "        self.block = RetNetBlock(retnetLayer, num_layers)\n",
    "        \n",
    "        self.output = torch.nn.Linear(embedding_dim, vocbal_size, device=device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.block(x)\n",
    "        #x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjHead(torch.nn.Module):\n",
    "    def __init__(self,in_dim = 640, hid_dim = 320, out_dim = 33, droupout = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim,hid_dim,bias=True),\n",
    "            torch.nn.Dropout(droupout,inplace=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hid_dim,out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self,combined_embedding):\n",
    "        outputs = self.layer(combined_embedding)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class  SARSCoV2ESM2(torch.nn.Module):\n",
    "    def __init__(self,esm2,retnet,isEval = False):\n",
    "        super(SARSCoV2ESM2, self).__init__()\n",
    "        self.esm2 = esm2\n",
    "        self.retnet = retnet\n",
    "        self.head = ProjHead()\n",
    "        self.proj = torch.nn.Linear(640,320)\n",
    "        self.isEval = isEval\n",
    "    def forward(self,x):\n",
    "        x_dict = {'input_ids': x.to(device), 'attention_mask': torch.ones(len(x), 225).to(device)}\n",
    "        esm_outputs = self.esm2(**x_dict).last_hidden_state\n",
    "        retnet_outputs = self.retnet(x.to(device))\n",
    "        combined_embedding = torch.cat((esm_outputs,retnet_outputs),dim=2)\n",
    "        \n",
    "        if self.isEval:\n",
    "            outputs = self.proj(combined_embedding)\n",
    "        else:\n",
    "            outputs = self.head(combined_embedding)\n",
    "        return outputs\n",
    "    \n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_sequences_dict = {}\n",
    "aa_sequences_list = []\n",
    "\n",
    "for record in SeqIO.parse('./rbd.fasta', 'fasta'):\n",
    "    seqId = record.description.split('|')[3]\n",
    "    aa_sequences_dict[record.seq] = [] \n",
    "\n",
    "    seq_info_dict = {\n",
    "        'seqId' : seqId,\n",
    "    }\n",
    "\n",
    "    aa_sequences_dict[record.seq].append(seq_info_dict)\n",
    "\n",
    "for seq in list(aa_sequences_dict.keys()):\n",
    "    aa_sequences_list.append(str(seq))\n",
    "\n",
    "seq_encode = tokenizer_(aa_sequences_list,return_tensors=\"pt\")[\"input_ids\"]\n",
    "seqs = [seq for seq in seq_encode]\n",
    "\n",
    "model = torch.load('./model_final.pth').to(device)\n",
    "batch_size = 1000\n",
    "data_loader = torch.utils.data.DataLoader(dataset=seq_encode, batch_size=batch_size)\n",
    "seq_embedding = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch_data in enumerate(data_loader):\n",
    "        output = model(batch_data)\n",
    "        seq_embedding.append(output)\n",
    "\n",
    "seq_embeddings = torch.cat(seq_embedding, dim=0)\n",
    "seq_embeddings = seq_embeddings.cpu().numpy()\n",
    "\n",
    "for id ,seq in enumerate(aa_sequences_dict):\n",
    "    for seq_info_dict in aa_sequences_dict[seq]:\n",
    "        seq_info_dict['embedding'] = seq_embeddings[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = list(SeqIO.parse('./cov2_wt.fasta', \"fasta\"))\n",
    "seqs_str = [str(record.seq) for record in records]\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "seq_encode = tokenizer_(seqs_str,return_tensors=\"pt\")[\"input_ids\"]\n",
    "seqs = [seq for seq in seq_encode]\n",
    "seq_wt = str(records[0].seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mutant(wt_seq, mutations):\n",
    "    mut_seq = wt_seq\n",
    "    for mutation in mutations:\n",
    "        aa_orig = mutation[0]\n",
    "        aa_pos = int(mutation[1:-1]) - 319\n",
    "        aa_mut = mutation[-1]\n",
    "        mut_seq = mut_seq[:aa_pos] + aa_mut + mut_seq[aa_pos + 1:]\n",
    "    return [mut_seq]\n",
    "\n",
    "def grammaticality_change(aa_pos_prob, seq, mutations,):\n",
    "    if len(mutations) == 0:\n",
    "        return 0\n",
    "    mut_probs = []\n",
    "    for mutation in mutations:\n",
    "        aa_orig = mutation[0]\n",
    "        aa_pos = int(mutation[1:-1]) - 319\n",
    "        aa_mut = mutation[-1]\n",
    "        if (seq[aa_pos] != aa_orig):\n",
    "            print(mutation)\n",
    "        assert(seq[aa_pos] == aa_orig)\n",
    "        mut_probs.append(aa_pos_prob[(aa_mut, aa_pos + 1)])\n",
    "\n",
    "    return np.mean(np.log10(mut_probs))\n",
    "\n",
    "def get_mutations(seq1, seq2):\n",
    "    mutations = []\n",
    "    alignment = pairwise2.align.globalms(seq1, seq2, 5, -4, -3, -.1, one_alignment_only=True,)[0]\n",
    "    pos = 0\n",
    "    for ch1, ch2 in zip(alignment[0], alignment[1]):\n",
    "        if ch1 != ch2 and ch1 != '-' and ch2 != '-':\n",
    "            mutations.append('{}{}{}'.format(ch1, pos + 319, ch2))\n",
    "        if ch1 != '-':\n",
    "            pos += 1\n",
    "    return mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<null_1>': 31, '<mask>': 32}\n",
    "model = torch.load('./model_final.pth')\n",
    "data_loader = torch.utils.data.DataLoader(dataset=seq_encode, batch_size=1)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch_data in enumerate(data_loader):\n",
    "        seq_wt_embdding = model(batch_data)\n",
    "layer_norm = torch.nn.LayerNorm((33))\n",
    "output = layer_norm(seq_wt_embdding.cpu())\n",
    "output = torch.softmax(output,dim=-1)\n",
    "\n",
    "aa_position_probility = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(seq_wt)):\n",
    "    for aa in vocabulary:\n",
    "        aa_id = vocabulary[aa]\n",
    "        prob = output[0][i, aa_id]\n",
    "        aa_position_probility[(aa, i + 1)] = prob.item()\n",
    "\n",
    "seq_wt_embdding = seq_wt_embdding[0].cpu().numpy()\n",
    "sorted_seqs = sorted(seq for seq in aa_sequences_dict)\n",
    "null_changes = np.array([abs(aa_sequences_dict[seq][0]['embedding'].mean(0) - seq_wt_embdding.mean(0)).sum() for seq in sorted_seqs] )\n",
    "null_grammar = np.array([grammaticality_change(aa_position_probility, seq_wt, get_mutations(seq_wt, seq)) for seq in sorted_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_changes, mut_gramms = [], []\n",
    "BA1_mutations = ['G339D','S371L','S373P','S375F','K417N','N440K','G446S','S477N','T478K','E484A','Q493R','G496S','Q498R','N501Y','Y505H']\n",
    "mut_seq = make_mutant(seq_wt, BA1_mutations)\n",
    "mut_seq_encode = tokenizer_(mut_seq,return_tensors=\"pt\")[\"input_ids\"]\n",
    "mut_data_loader = torch.utils.data.DataLoader(dataset=mut_seq_encode, batch_size=1)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch_data in enumerate(mut_data_loader):\n",
    "        mut_seqembdding = model(batch_data)\n",
    "mut_seqembdding = mut_seqembdding[0].cpu().numpy()\n",
    "mut_change = abs(mut_seqembdding.mean(0) - seq_wt_embdding.mean(0)).sum()\n",
    "mut_changes.append(mut_change)\n",
    "\n",
    "mut_gramm = grammaticality_change(aa_position_probility, seq_wt, BA1_mutations)\n",
    "mut_gramms.append(mut_gramm)\n",
    "print('{}: Grammar percentile = {}%'.format(\"Omicron_BA.1\", ss.percentileofscore(null_grammar, mut_gramm)))\n",
    "print('{}: Change percentile = {}%'.format(\"Omicron_BA.1\", ss.percentileofscore(null_changes, mut_change)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mutation in BA1_mutations:\n",
    "    mut_seq = make_mutant(seq_wt, [ mutation ])\n",
    "    mut_encode = tokenizer_(mut_seq,return_tensors=\"pt\")[\"input_ids\"]\n",
    "    mut_data_loader = torch.utils.data.DataLoader(dataset=mut_encode, batch_size=1)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(mut_data_loader):\n",
    "            mut_seqembdding = model(batch_data)\n",
    "        mut_seqembdding = mut_seqembdding[0].cpu().numpy()\n",
    "        change = abs(mut_seqembdding.mean(0) - seq_wt_embdding.mean(0)).sum()\n",
    "        gramm = grammaticality_change(aa_position_probility, seq_wt, [mutation])\n",
    "        print('\\tMutation {}: change = {}, percentile = {}%'.format(mutation, change,ss.percentileofscore(null_changes, change)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "ax = sns.violinplot(data=null_changes, inner=None, color='white', width=0.7)\n",
    "ax = sns.stripplot(data=null_changes, color='#aaaaaa', size=1)\n",
    "#ax.scatter([0]*len(mut_changes), mut_changes, color='darkred') \n",
    "ax.annotate(\"Omicron_BA.1\", (0.002, mut_changes[0]))\n",
    "ax.get_xaxis().set_visible(False)\n",
    "plt.ylabel('Semantic change')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_changes, mut_gramms = [], []\n",
    "EG5_mutations = ['S477N','Y505H','K417N','N460K','D405N','R408S','T376A','S375F','S371F','T478K','F486S','E484A','F490S','N501Y','G446S','V445P','R346T','Q498R','N440K','S373P','G339H','L368I']\n",
    "mut_seq = make_mutant(seq_wt, EG5_mutations)\n",
    "mut_seq_encode = tokenizer_(mut_seq,return_tensors=\"pt\")[\"input_ids\"]\n",
    "mut_data_loader = torch.utils.data.DataLoader(dataset=mut_seq_encode, batch_size=1)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch_data in enumerate(mut_data_loader):\n",
    "        mut_seqembdding = model(batch_data)\n",
    "mut_seqembdding = mut_seqembdding[0].cpu().numpy()\n",
    "mut_change = abs(mut_seqembdding.mean(0) - seq_wt_embdding.mean(0)).sum()\n",
    "mut_changes.append(mut_change)\n",
    "\n",
    "mut_gramm = grammaticality_change(aa_position_probility, seq_wt, EG5_mutations)\n",
    "mut_gramms.append(mut_gramm)\n",
    "print('{}: Grammar percentile = {}%'.format(\"Omicron_EG.5\", ss.percentileofscore(null_grammar, mut_gramm)))\n",
    "print('{}: Change percentile = {}%'.format(\"Omicron_EG.5\", ss.percentileofscore(null_changes, mut_change)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mutation in EG5_mutations:\n",
    "    mut_seq = make_mutant(seq_wt, [ mutation ])\n",
    "    mut_encode = tokenizer_(mut_seq,return_tensors=\"pt\")[\"input_ids\"]\n",
    "    mut_data_loader = torch.utils.data.DataLoader(dataset=mut_encode, batch_size=1)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(mut_data_loader):\n",
    "            mut_seqembdding = model(batch_data)\n",
    "        mut_seqembdding = mut_seqembdding[0].cpu().numpy()\n",
    "        change = abs(mut_seqembdding.mean(0) - seq_wt_embdding.mean(0)).sum()\n",
    "        gramm = grammaticality_change(aa_position_probility, seq_wt, [mutation])\n",
    "        print('\\tMutation {}: change = {}, percentile = {}%'.format(mutation, change,ss.percentileofscore(null_changes, change)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
